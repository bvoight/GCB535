{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCB5350: Parsing and Quality Control of Data with awk and in UNIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many bioinformatic scientific endeavors, you will encounter data that has **gone wrong**: there are things that you need to fix with the data file *before* you load that data into R (and start tidyverse manipulation). \n",
    "\n",
    "A running 'joke' is that >75% of your time as an bioinformatician is actually spend formatting / processing data to have it \"work\" with an analytical tool!\n",
    "\n",
    "In this module, we want to give you some experience / examples looking at data, seeing issues that are awry, and then using your newly minted capabilities with command line tools to create pipelines that convert \"raw, messy\" data to \"slightly processed but less messy\" versions of those data.\n",
    "\n",
    "Remember: The goal here is NOT to edit the actual raw data file with a text editor. Often times, data sets are simply too large to do this feasibly. In addition, keeping a \"raw\" unedited version of data *sacrosanct* is essential purposes of reproducibility: when attempting computational validation of a result, computational scientists will often want to start from a common \"starting point\" (i.e., a file the downloaded from the public domain) and get to the same place as your pipelines do. As such, building pipelines that start from that common, very raw (and messy) data file really helps with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helpful tips that might be useful to you in the work below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. $0 in awk.** As we discussed in the lecture, there are variables in awk that are helpful to refer to the entries that it parses as it read through lines. to remind you:\n",
    "\n",
    "```bash\n",
    "$1\n",
    "```\n",
    "Refers to the \"first column\" after awk 'splits' the line. However, you can also just get the entire line (without spliting) with: \n",
    "\n",
    "```bash\n",
    "$0\n",
    "```\n",
    "\n",
    "For example, these two lines will print the same thing:\n",
    "```bash\n",
    "$ awk ' {print($0)} ' <mydata.txt\n",
    "$ awk ' {print} ' <mydata.txt\n",
    "```\n",
    "\n",
    "However, in the first case, you can append things if you wanted:\n",
    "```bash\n",
    "$ awk ' {print(\">>>\",$0)} ' <mydata.txt\n",
    "```\n",
    "would print \">>>\" before every line.\n",
    "\n",
    "**II. Number of Fields (`NF`) and Number of lines Read (`NR`).** awk also knows how many entries were parsed for the given line and keeps a running counter for the number of lines read. You can access them with the variables `NF` and `NR`. You can use `NF` and `NR` in conditional statements or as terms to print out:\n",
    "\n",
    "```bash\n",
    "$ awk ' NF > 2 {print(NR)} ' <mydata.txt\n",
    "```\n",
    "This will print the number of the line if the number of entries in the line is greater than two.\n",
    "\n",
    "**III. Nesting conditional statements.** While it can be tricky, awk can handle multiple conditional statements and uses logical operators similar to what you have learned in R. For example:\n",
    "\n",
    "```bash\n",
    "$ awk ' (NR > 10 && NR < 15) || (NR > 25) {print} ' <mydata.txt\n",
    "```\n",
    "will print lines 11-14 plus lines 26 and onward in the file.\n",
    "\n",
    "**IV. Pattern matching.** awk can also match patterns (strings) that you want it do using `~ //` syntax. For example\n",
    "\n",
    "```bash\n",
    "$ awk ' $1 ~ /fleas/ {print} ' <mydata.txt\n",
    "```\n",
    "will look if the string `fleas` is present in the first entry of each line, and will print that line if it finds it. \n",
    "\n",
    "You could also use `$0` if you were looking for it anywhere in the line:\n",
    "\n",
    "```bash\n",
    "$ awk ' $0 ~ /fleas/ {print} ' <mydata.txt\n",
    "```\n",
    "\n",
    "Or if actually you didn't want fleas at all, you can negate this with the shebang `!~`:\n",
    "```bash \n",
    "$ awk ' $0 !~ /fleas/ {print(\"no fleas, please!\")} ' <mydata.txt\n",
    "```\n",
    "\n",
    "You can also apply conditions if you wanted:\n",
    "\n",
    "```bash\n",
    "$ awk ' $0 !~ /fleas/ {print(\"Doxx fleas!\")} $0 ~ /fleabag/ {print(\"I <3 Phoebe Waller-Bridge\")} ' <mydata.txt\n",
    "```\n",
    "\n",
    "\n",
    "**V. Reminder tips for grep.** In case you need, check out these flags\n",
    "\n",
    "```bash\n",
    "$ grep -n\n",
    "$ grep -A 10\n",
    "$ grep -B 20\n",
    "```\n",
    "`-n` will print the line number that matches\n",
    "\n",
    "`-A 10` will print the number of lines after you find a match, in this case 10\n",
    "\n",
    "`-B 20` will print the number of lines that preceed a match, in this case 20\n",
    "\n",
    "**###**\n",
    "\n",
    "With this in mind, work through the following \"cases\" below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Your research assistant has somehow 'accidentally' managed to do some very strange things to a .fasta data file of core core circadian clock gene sequences in a novel model organism that you have generated (Madeupicus remediare).\n",
    "\n",
    "**core_clock_sequences.fasta**\n",
    "\n",
    "Unfortunately, these data haven't been uploaded to NCBI, and there's no backup of the data! \n",
    "\n",
    "So, you are going to need to take a look at the file and figure out what has gone wrong.\n",
    "\n",
    "Luckily, you know that:\n",
    "* The sequence of genes you collected were the same ones listed in **my_clock_genelist.txt**\n",
    "* The list of gene sequences were only DNA sequences\n",
    "\n",
    "In the following question:\n",
    "\n",
    "(Part 1) Report (in human terms) each issue you uncover and have to fix for the .fasta file. \n",
    "\n",
    "(Part 2) Use UNIX code to fix **one** of the issues you observed. Totally OK to create an intermediate file that leaves some issues unresolved -- but addresses the problem you identified.\n",
    "\n",
    "**Note:** If you find multiple issues and feel inclined to solve them, grand! Update your pipeline accordingly in a step-wise, describe the issues you solved and then sequential fix them in Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Part 1)** Explanation (issues in each file, each issue should be described 10 words or less):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Part 2)** Correction step Code + Result:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Your \"cat\" walked on your keyboard during a ZOOM meeting and somehow managed to corrupt a bed file that you are currently working on. The corrupted file is `CpG_isl_data.bed` which indicates CpG islands in genome. The file is found in your working directory.\n",
    "\n",
    "Unfortunately, you realize you have no back-ups for the original data!\n",
    "\n",
    "Fortunately, you remember corrected that the original file was a standard, 3 column BED formatted file.\n",
    "\n",
    "The structure of BED files can be found [here](https://useast.ensembl.org/info/website/upload/bed.html). \n",
    "\n",
    "In the following question:\n",
    "\n",
    "(Part 1) Report (in human terms) each issue you uncover and have to fix for each .bed file.\n",
    "\n",
    "(Part 2) Use UNIX code to create a \"quality control (QC) pipeline\" - a series of steps that work to progessively make the file \"better\". The pipeline should:\n",
    "\n",
    "* Correct the issue. OK to work sequentially and create intermediates (you might have multiples of these)\n",
    "* Creates a final file with all of these issues \"fixed\".\n",
    "\n",
    "Here, you to make sure all lines of code and the steps can be \"reproduced\" by copy-pasting and re-executing code that you have created, starting with the original version of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(Part 1)** Explanation (issues in each file, each issue should be described 10 words or less):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(Part 2)** Quality Control Pipeline; Code + Result:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  },
  "name": "Sagemathcloud_Setup_Assignment.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
